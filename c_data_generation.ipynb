{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate NER data and bootstrap\n",
    "\n",
    "clean up, create NER output\n",
    "\n",
    "use wordlist to duplicate sentences to supplement training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from data.preprocessing import get_vocab, index_sents\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/merged_corpus_phrases_new_fixed.csv')\n",
    "data = pd.read_csv('data/merged_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to str due to number-only lines\n",
    "sents = [str(s) for s in data['message'].tolist()]\n",
    "speechacts = data['speech_act'].tolist()\n",
    "topics = [str(s) for s in data['topic'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix 'p m'\n",
    "fixed = []\n",
    "for idx, sent in enumerate(sents):\n",
    "    sent = sent.replace(' p m', ' pm')\n",
    "    sent = sent.replace(' p m.', ' pm.')\n",
    "    sent = sent.replace(' p m?', ' pm?')\n",
    "    sent = sent.replace(' p m,', ' pm,')\n",
    "    sent = sent.replace(' p m ', ' pm ')\n",
    "    sent = sent.replace(' a m.', ' am.')\n",
    "    sent = sent.replace(' a m?', ' am?')\n",
    "    sent = sent.replace(' a m,', ' am,')\n",
    "    sent = sent.replace(' a m ', ' am ')\n",
    "    sent = re.sub('a\\sm$', 'am', sent)\n",
    "    fixed.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pos-tag with NLTK\n",
    "postagged = [pos_tag(word_tokenize(s)) for s in fixed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 4.349464036041634)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(l) for l in postagged]\n",
    "max(lens), (sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 'JJ'), ('afternoon', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('San', 183),\n",
       " ('Francisco', 114),\n",
       " ('United', 106),\n",
       " ('Euston', 95),\n",
       " ('American', 95),\n",
       " ('Saturday', 75),\n",
       " ('Virgin', 73),\n",
       " ('October', 73),\n",
       " ('Monday', 67),\n",
       " ('Sunday', 60)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "for sent in fixed:\n",
    "    words = re.findall(r'[A-Z][a-z]+', sent)\n",
    "    for word in words:\n",
    "        names.append(word)\n",
    "\n",
    "nameCounts = Counter(names)\n",
    "nameCounts.most_common(10) # remove 10 to see all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('US', 39),\n",
       " ('WWW', 27),\n",
       " ('LA', 12),\n",
       " ('SFO', 9),\n",
       " ('TWA', 9),\n",
       " ('LAX', 8),\n",
       " ('JFK', 8),\n",
       " ('DDD', 7),\n",
       " ('CC', 6),\n",
       " ('CCC', 6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "for sent in fixed:\n",
    "    words = re.findall(r'[A-Z][A-Z]+', sent)\n",
    "    for word in words:\n",
    "        names.append(word)\n",
    "\n",
    "nameCounts = Counter(names)\n",
    "nameCounts.most_common(10) # remove 10 to see all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER auto-tagging\n",
    "\n",
    "this code is messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lists\n",
    "timewords = ['midnight', 'noon', 'am', 'pm', 'morning', 'afternoon', 'evening', 'night']\n",
    "\n",
    "datewords = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'January', 'February',\n",
    "             'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
    "             'tomorrow']\n",
    "\n",
    "# from transdict finds\n",
    "placewords = ['Euston', 'London', 'Birmingham', 'Street', 'Chicago', 'Wilmslow', 'Macclesfield', 'Stockport',\n",
    "              'Piedmont', 'Dallas', 'Newark', 'Wigan', 'Preston', 'Denver', 'Liverpool', 'Seattle', 'Tokyo',\n",
    "              'Wrexham', 'Richmond' , 'Manchester', 'Crewe', 'Baltimore', 'Ottawa', 'Toronto', 'Vancouver', \n",
    "              'Moscow', 'Boston', 'Edinburgh', 'Oakland', 'Newcastle', 'Durham', 'Lime', 'Taunton', \n",
    "              'Copenhagen', 'Heathrow', 'Helsinki', 'Pittsburg', 'Raleigh', 'Picadilly', 'Watford', 'Hertford',\n",
    "              'Leicester', 'Newton', 'Abbot', 'Greenbay', 'Miami', 'Orlando', 'Washington', 'Dulles', 'Charlotte',\n",
    "              'Tahoe', 'Southbend', 'Springfield', 'York', 'Burbank', 'Syracuse', 'Cleveland', 'Fairbanks',\n",
    "              'Carolina', 'Montreal', 'Wolverhampton', 'Leeds', 'Derby', 'Blackpool', 'Oxenholme', 'Ontario',\n",
    "              'Riyadh', 'Portland', 'Barclay', 'Calgary', 'Bangkok', 'Burigan', 'Nantucket', 'Menlo', 'Cottage',\n",
    "              'Wisconsin', 'Gatwick', 'Singapore', 'Irvine', 'Frankfurt', 'Jersey', 'Columbus', 'Merseyside', \n",
    "              'Fanshawe', 'Essex', 'Stafford', 'Philadelphia', 'Switzerland', 'Denmark', 'Sandestrom',\n",
    "              'Braniff', 'Stockholm', 'Sweden', 'Germany', 'Belgium', 'Brussels', 'Rochester', 'Anchorage', \n",
    "              'California', 'Arkansas', 'England', 'Michigan', 'Detroit', 'Indiana', 'Louisville', 'Ohio',\n",
    "              'Tulsa', 'Indianapolis', 'Milwaukee', 'Oklahoma', 'Colorado', 'Virginia', 'Coxhoe', 'Redwich',\n",
    "              'Camden', 'Leicestershire', 'Cumbria', 'Heswall', 'Wirral', 'Liver', 'Cheshire', 'Goldhampton',\n",
    "              'International', 'Airport', 'Central', 'General', 'St', 'Atlanta', 'Nuneaton', 'Beijing', 'Europe',\n",
    "              'LA', 'ORD', 'SFO', 'JFK', 'LAX', 'SAS', 'SF', 'Pancreas', 'Rugby']\n",
    "\n",
    "# from transdict finds\n",
    "companywords = ['United', 'American', 'Virgin', 'Express', 'Delta', 'Trainlines', 'Visa', 'Airlines', 'Travel',\n",
    "                'Hertz', 'Lufthansa', 'Northwest', 'Canadian', 'British', 'Trainline', 'Northwestern',\n",
    "                'Mastercard', 'Airline', 'Southwestern', 'Telesales', 'Merchandising', 'Aeroflight',\n",
    "                'Airways', 'Korean', 'Hotel', 'Alaskan', 'Alaska', 'TWA', 'Eagle', 'Saudia', 'Arabian', \n",
    "                'PanAm', 'CP', 'Air', 'trainlines', 'Continental']\n",
    "\n",
    "# seat-class words\n",
    "ticketwords = ['Coach', 'Business', 'non-stop', 'Round-trip', 'Advance',\n",
    "               'Super', 'Value', 'Return', 'Saver', 'First', 'Class', 'Single']\n",
    "\n",
    "# names\n",
    "namewords = ['Sandra']\n",
    "\n",
    "# if number + these = time\n",
    "timeparts = [\"o'clock\"]\n",
    "\n",
    "# split names\n",
    "placefirsts = ['Palo', 'Green', 'New', 'Lime', 'Orange', 'San', 'Los', 'Hong', 'John', 'Las', 'Saudi', 'Saint',\n",
    "               'Santa', 'Soviet', 'Little']\n",
    "placelasts = ['Alto', 'Bay', 'Lime' 'Street', 'County', 'Francisco', 'Angeles', 'Diego', 'Jose', 'Bernadino',\n",
    "              'Kong', 'Wayne', 'Vegas', 'Arabia', 'Louis', 'Petersburg', 'Ana', 'Union', 'Rock']\n",
    "\n",
    "compfirsts = ['Cathay', 'Pan']\n",
    "complasts = ['Pacific', 'Am']\n",
    "\n",
    "currencies = ['pounds', 'dollars', 'cents', 'dollar']\n",
    "\n",
    "numbers = ['hundred', 'hundreds', 'thousand', 'thousands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetag = 'TIM'\n",
    "datetag = 'DAT'\n",
    "citytag = 'GEO'\n",
    "comptag = 'COM'\n",
    "classtag = 'TIX'\n",
    "flighttag = 'FLT'\n",
    "currtag = 'CUR'\n",
    "numtag = 'NUM'\n",
    "\n",
    "# to save\n",
    "alltoks = []\n",
    "alltags = []\n",
    "allners = []\n",
    "\n",
    "# to print\n",
    "makecsv = []\n",
    "\n",
    "indices = []\n",
    "\n",
    "# iterate\n",
    "for jdx, seq in enumerate(postagged):\n",
    "    \n",
    "    # initialize\n",
    "    toks = [t[0] for t in seq]\n",
    "    tags = [t[1] for t in seq]\n",
    "    ners = ['O' for i in range(len(toks))]\n",
    "\n",
    "    # iterate and list-replace\n",
    "    for idx, tok in enumerate(toks[:-1]):\n",
    "        # time words using list substitution\n",
    "        if tok in timewords:\n",
    "            ners[idx] = timetag\n",
    "        # date words\n",
    "        elif tok in datewords:\n",
    "            ners[idx] = datetag\n",
    "        # date words\n",
    "        elif tok in currencies:\n",
    "            ners[idx] = currtag\n",
    "        # place names using list substitution\n",
    "        elif tok in placewords:\n",
    "            ners[idx] = citytag\n",
    "        # company names using list substitution\n",
    "        elif tok in companywords:\n",
    "            ners[idx] = comptag\n",
    "        # ticket class words\n",
    "        elif tok in ticketwords:\n",
    "            ners[idx] = classtag\n",
    "        \n",
    "        # o'clock time using regex\n",
    "        if re.match(r'[0-9]+', tok):\n",
    "            if toks[idx+1] in timeparts:\n",
    "                ners[idx] = timetag\n",
    "                ners[idx+1] = timetag\n",
    "         \n",
    "        # try flight numbers\n",
    "        if tok == 'flight' and re.findall(r'[0-9]', toks[idx+1]):\n",
    "            ners[idx] = flighttag\n",
    "        \n",
    "        # time re search\n",
    "        if re.findall(r'[0-9]+\\:[0-9]+(\\.\\,\\?)*', tok):\n",
    "            ners[idx] = timetag    \n",
    "        \n",
    "        # else assume flight number unless before prep\n",
    "        if ners[idx] == 'O' and re.match(r'[0-9]+', tok):\n",
    "            if toks[idx+1] in ['hours','minutes','miles','kilometers', 'person', 'persons', 'people', 'members']:\n",
    "                ners[idx] = numtag\n",
    "            elif toks[idx+1] in currencies:\n",
    "                ners[idx] = currtag\n",
    "            elif toks[idx+1] in timewords or toks[idx+1] in timeparts:\n",
    "                ners[idx] = timetag\n",
    "            elif idx > 0:\n",
    "                if toks[idx-1] in currencies:\n",
    "                    ners[idx] = currtag\n",
    "                elif ners[idx-1] == flighttag or ners[idx-1] == comptag:\n",
    "                    ners[idx] = flighttag\n",
    "                else:\n",
    "                    ners[idx] = numtag\n",
    "            else:\n",
    "                ners[idx] = numtag\n",
    "        \n",
    "        # ordinal numbers for dates\n",
    "        if re.findall(r'[0-9]+(nd|st|th|rd)', tok):\n",
    "            ners[idx] = datetag\n",
    "                \n",
    "        # two-word place names\n",
    "        if tok in placefirsts:\n",
    "            if toks[idx+1] in placelasts:\n",
    "                ners[idx] = citytag\n",
    "                ners[idx+1] = citytag\n",
    "                \n",
    "        # two-word company names\n",
    "        if tok in compfirsts:\n",
    "            if toks[idx+1] in complasts:\n",
    "                ners[idx] = comptag\n",
    "                ners[idx+1] = comptag\n",
    "                \n",
    "        # ticket class words\n",
    "        elif tok in timeparts:\n",
    "            ners[idx-1] = timetag\n",
    "            ners[idx] = timetag\n",
    "                \n",
    "    # last word\n",
    "    # time words using list substitution\n",
    "    if toks[-1] in timewords:\n",
    "        ners[-1] = timetag\n",
    "    # date words\n",
    "    elif toks[-1] in datewords:\n",
    "        ners[-1] = datetag\n",
    "    elif toks[-1] in numbers:\n",
    "        ners[-1] = numtag\n",
    "    elif toks[-1] in currencies:\n",
    "        ners[-1] = currtag\n",
    "    # place names using list substitution\n",
    "    elif toks[-1] in placewords:\n",
    "        ners[-1] = citytag\n",
    "    # company names using list substitution\n",
    "    elif toks[-1] in companywords:\n",
    "        ners[-1] = comptag\n",
    "    # ticket class words\n",
    "    elif toks[-1] in ticketwords:\n",
    "        ners[-1] = classtag\n",
    "    # time re search\n",
    "    if re.match(r'[0-9]+\\:[0-9]+(\\.\\,\\?)*', toks[-1]):\n",
    "        ners[-1] = timetag \n",
    "    # ordinal numbers for dates\n",
    "    elif re.findall(r'[0-9]+(nd|st|th|rd)', toks[-1]):\n",
    "        ners[-1] = datetag\n",
    "    elif len(seq) > 1 and re.match(r'[0-9]+', toks[-1]): \n",
    "        if ners[-2] == flighttag or ners[-2] == numtag:\n",
    "            ners[-1] = ners[-2]\n",
    "        elif toks[-1] in currencies:\n",
    "            ners[-1] = currtag\n",
    "        elif tags[-2] == 'IN':\n",
    "            ners[-1] = timetag\n",
    "        else:\n",
    "            ners[-1] = numtag\n",
    "    \n",
    "    # final pass\n",
    "    for i in range(len(toks)):\n",
    "        if i < len(toks)-1:\n",
    "            if toks[i] == 'good' and toks[i+1] == 'morning' or toks[i+1] == 'afternoon':\n",
    "                ners[i+1] = 'O'        \n",
    "    \n",
    "    answ = pd.DataFrame(\n",
    "    {\n",
    "        'toks': toks,\n",
    "        'tags': tags,\n",
    "        'ners': ners,\n",
    "        'skip' : [' ' for s in toks]\n",
    "    })\n",
    "\n",
    "    answ = answ[['toks', 'ners', 'skip']]\n",
    "    answ = answ.T\n",
    "    \n",
    "    # add to test data\n",
    "    alltoks.append(toks)\n",
    "    alltags.append(tags)\n",
    "    allners.append(ners)\n",
    "    indices.append(jdx)  # save sentence index\n",
    "    makecsv.append(answ) # append df to list seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2127),\n",
       " ('to', 1592),\n",
       " ('you', 1487),\n",
       " (\"'s\", 1440),\n",
       " ('i', 1364),\n",
       " ('that', 1314),\n",
       " ('and', 1304),\n",
       " ('it', 1062),\n",
       " ('ok', 1008),\n",
       " ('a', 913)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [w for s in alltoks for w in s]\n",
    "vcounts = Counter(vocab)\n",
    "vcounts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(makecsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6677"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in postagged if len(x)> 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER training prototype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12874, 12874, 12874, 12874)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speechacts), len(alltoks), len(alltags), len(allners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saCounter = Counter(speechacts)\n",
    "len(set(speechacts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65,\n",
       " [('state', 2267),\n",
       "  ('acknowledge', 2186),\n",
       "  ('refer', 1706),\n",
       "  ('reqInfo', 1060),\n",
       "  ('init', 999),\n",
       "  ('hesitate', 518),\n",
       "  ('reqConfirm', 411),\n",
       "  ('abandon', 372),\n",
       "  ('hold', 293),\n",
       "  ('negate', 188),\n",
       "  ('thank', 186),\n",
       "  ('stateIntent', 181),\n",
       "  ('exclaim', 179),\n",
       "  ('stateCondition', 179),\n",
       "  ('stateConstraint', 173),\n",
       "  ('approve', 170),\n",
       "  ('bye', 166),\n",
       "  ('predict', 163),\n",
       "  ('expressOpinion', 149),\n",
       "  ('reqDirect', 146),\n",
       "  ('agree', 126),\n",
       "  ('expressPossibility', 112),\n",
       "  ('identifySelf', 104),\n",
       "  ('greet', 96),\n",
       "  ('suggest', 76),\n",
       "  ('direct', 64),\n",
       "  ('stateReason', 62),\n",
       "  ('apologise', 57),\n",
       "  ('expressWish', 40),\n",
       "  ('pardon', 36),\n",
       "  ('stateOpt', 33),\n",
       "  ('reqOpt', 32),\n",
       "  ('confirm', 26),\n",
       "  ('uninterpretable', 26),\n",
       "  ('expressNonAwareness', 25),\n",
       "  ('expressImPossibility', 24),\n",
       "  ('echo', 24),\n",
       "  ('correct', 22),\n",
       "  ('accept', 20),\n",
       "  ('phatic', 19),\n",
       "  ('correctSelf', 19),\n",
       "  ('offer', 18),\n",
       "  ('expressRegret', 16),\n",
       "  ('expressAwareness', 16),\n",
       "  ('complete', 12),\n",
       "  ('reject', 12),\n",
       "  ('elab', 11),\n",
       "  ('expressSurprise', 9),\n",
       "  ('referReason', 6),\n",
       "  ('predictPossibility', 5),\n",
       "  ('enumeration', 4),\n",
       "  ('add', 4),\n",
       "  ('reqModal', 4),\n",
       "  ('expressConviction', 3),\n",
       "  ('referCondition', 3),\n",
       "  ('refuse', 3),\n",
       "  ('answer', 2),\n",
       "  ('referOpt', 2),\n",
       "  ('promise', 2),\n",
       "  ('contrast', 2),\n",
       "  ('expressUnCertainty', 1),\n",
       "  ('suggestOpt', 1),\n",
       "  ('disapprove', 1),\n",
       "  ('reportOpinion', 1),\n",
       "  ('echorefer', 1)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the speech-act prefixes\n",
    "truncsa = [sa.split('-')[-1] for sa in speechacts]\n",
    "tsaCounter = Counter(truncsa)\n",
    "len(set(truncsa)), tsaCounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(705,\n",
       " 40,\n",
       " [('none', 7868),\n",
       "  ('enum', 1201),\n",
       "  ('time', 487),\n",
       "  ('day', 457),\n",
       "  ('location', 395),\n",
       "  ('number', 332),\n",
       "  ('arrival', 239),\n",
       "  ('journey', 188),\n",
       "  ('airline', 176),\n",
       "  ('creditcard', 164),\n",
       "  ('date', 152),\n",
       "  ('from', 137),\n",
       "  ('name', 123),\n",
       "  ('verify', 104),\n",
       "  ('seat', 104),\n",
       "  ('fare', 90),\n",
       "  ('departure', 84),\n",
       "  ('address', 79),\n",
       "  ('availability', 68),\n",
       "  ('month', 58),\n",
       "  ('railcard', 53),\n",
       "  ('cancel', 49),\n",
       "  ('booking', 48),\n",
       "  ('return', 42),\n",
       "  ('district', 32),\n",
       "  ('to', 30),\n",
       "  ('confirm', 22),\n",
       "  ('problem', 17),\n",
       "  ('miss', 14),\n",
       "  ('airport', 13),\n",
       "  ('hotel', 12),\n",
       "  ('nan', 9),\n",
       "  ('refund', 7),\n",
       "  ('duration', 6),\n",
       "  ('car', 6),\n",
       "  ('week', 3),\n",
       "  ('country', 2),\n",
       "  ('spell', 1),\n",
       "  ('disflu', 1),\n",
       "  ('penalty', 1)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the topic prefixes, suffixes\n",
    "tpCounter = Counter(topics)\n",
    "trunctop = [t.split('-')[-1] for t in topics]\n",
    "trunctop = [t.split('_')[0] for t in trunctop]\n",
    "trunctopCounter = Counter(trunctop)\n",
    "len(set(topics)), len(set(trunctop)), trunctopCounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicSA = [truncsa[i]+'-'+trunctop[i] for i in range(len(topics))]\n",
    "topicSACounter = Counter(topicSA)\n",
    "len(set(topicSA)) #, topicSACounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by length, remove all of length 2 or under\n",
    "indices = []\n",
    "for idx, tokseq in enumerate(alltoks):\n",
    "    if len(tokseq) > 2:\n",
    "        indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortTops = [topicSA[i] for i in indices]\n",
    "shortTopCounter = Counter(shortTops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_speechacts = [truncsa[i] for i in indices]\n",
    "short_topics = [trunctop[i] for i in indices]\n",
    "short_topic_acts = shortTops[:]\n",
    "short_tokens = [alltoks[i] for i in indices]\n",
    "short_postags = [alltags[i] for i in indices]\n",
    "short_nertags = [allners[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "          'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "cities = ['Chicago', 'Wilmslow', 'Macclesfield', 'Stockport',\n",
    "          'Piedmont', 'Dallas', 'Newark', 'Wigan', 'Preston', 'Denver', 'Liverpool', 'Seattle', 'Tokyo',\n",
    "          'Wrexham', 'Richmond' , 'Manchester', 'Crewe', 'Baltimore', 'Ottawa', 'Toronto', 'Vancouver', \n",
    "          'Moscow', 'Boston', 'Edinburgh', 'Oakland', 'Newcastle', 'Durham', 'Taunton', \n",
    "          'Copenhagen', 'Helsinki', 'Pittsburg', 'Raleigh', 'Picadilly', 'Watford',\n",
    "          'Leicester', 'Newton', 'Abbot', 'Greenbay', 'Miami', 'Orlando', 'Charlotte',\n",
    "          'Tahoe', 'Southbend', 'Springfield', 'Burbank', 'Syracuse', 'Cleveland', 'Fairbanks',\n",
    "          'Montreal', 'Wolverhampton', 'Leeds', 'Derby', 'Blackpool', 'Oxenholme', 'Ontario',\n",
    "          'Riyadh', 'Portland', 'Barclay', 'Calgary', 'Bangkok', 'Burigan', 'Nantucket', 'Menlo', 'Cottage',\n",
    "          'Gatwick', 'Singapore', 'Irvine', 'Frankfurt', 'Jersey', 'Columbus', 'Merseyside', \n",
    "          'Fanshawe', 'Essex', 'Stafford', 'Philadelphia', 'Sandestrom',\n",
    "          'Braniff', 'Stockholm', 'Brussels', 'Rochester', 'Anchorage', \n",
    "          'Detroit', 'Indiana', 'Louisville',\n",
    "          'Tulsa', 'Indianapolis', 'Milwaukee', 'Oklahoma', 'Coxhoe', 'Redwich',\n",
    "          'Camden', 'Leicestershire', 'Cumbria', 'Heswall', 'Wirral', 'Liver', 'Goldhampton',\n",
    "          'Atlanta', 'Nuneaton', 'Beijing']\n",
    "\n",
    "newcities = ['Shanghai', 'Seoul', 'Busan', 'Daegu', 'Paris', 'Istanbul', 'Delhi', 'Mumbai', 'Cairo',\n",
    "             'Tehran', 'Ankara', 'Berlin', 'Madrid', 'Incheon', 'Rome', 'Osaka',\n",
    "             'Nagoya', 'Manila', 'Minsk', 'Budapest', \"Warsaw\", \"Phoenix\"]\n",
    "\n",
    "airlines = ['United', 'Virgin', 'Delta', 'Trainlines',\n",
    "            'Hertz', 'Lufthansa', 'Northwest', 'Canadian', 'British', 'Northwestern',\n",
    "            'Airline', 'Southwestern', 'Aeroflight', 'Continental',\n",
    "            'Airways', 'Korean', 'Alaskan', 'Alaska', 'TWA', 'Eagle', 'Saudia', 'Arabian', \n",
    "            'PanAm', 'CP', 'Air']\n",
    "\n",
    "newairlines = ['Allegiant', 'Frontier', 'JetBlue', 'Spirit', 'Envoy', 'SkyWest', 'Quantas',\n",
    "               'Etihad', 'Dragonair', 'AirAsia', 'Finnair', 'Aegean', 'easyJet', 'WestJet',\n",
    "               'Indigo', 'Iberia', 'Emirates', 'EVA', 'ANAs', 'JAL']\n",
    "\n",
    "journey = ['journey', 'trip', 'itinerary']\n",
    "\n",
    "traveling = ['travelling', 'traveling', 'flying']\n",
    "\n",
    "depart = ['depart', 'leave']\n",
    "\n",
    "departing = ['departing', 'leaving']\n",
    "\n",
    "book = ['book', 'schedule', 'reserve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0 of 6677\n",
      "done 1000 of 6677\n",
      "done 2000 of 6677\n",
      "done 3000 of 6677\n",
      "done 4000 of 6677\n",
      "done 5000 of 6677\n",
      "done 6000 of 6677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8838, 8838, 8838, 8838, 8838, 8838)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_speechacts = []\n",
    "add_topics = []\n",
    "add_topic_acts = []\n",
    "add_tokens = []\n",
    "add_postags = []\n",
    "add_nertags = []\n",
    "\n",
    "for i in range(len(short_speechacts)):\n",
    "    \n",
    "    iters = np.random.randint(4, 8)\n",
    "    for j in range(iters):\n",
    "    \n",
    "        toks = short_tokens[i]\n",
    "        tags = short_postags[i]\n",
    "        ners = short_nertags[i]\n",
    "\n",
    "        new_toks = toks[:]\n",
    "\n",
    "        # check each list\n",
    "        for lst in [days, months, cities, airlines, journey, traveling, depart, departing, book]:\n",
    "\n",
    "            # check each word\n",
    "            for idx, tok in enumerate(toks):\n",
    "\n",
    "                # if found, randomly sample new word\n",
    "                if tok in lst:\n",
    "                    new = tok\n",
    "                    while new == tok:\n",
    "                        if lst == cities:\n",
    "                            new = newcities[np.random.randint(0, len(newcities))]\n",
    "                        elif lst == airlines:\n",
    "                            new = newairlines[np.random.randint(0, len(newairlines))]\n",
    "                        else:\n",
    "                            new = lst[np.random.randint(0, len(lst))]\n",
    "                    # replace\n",
    "                    new_toks[idx] = new\n",
    "        \n",
    "        # update if new sent\n",
    "        if toks != new_toks:\n",
    "            add_tokens.append(new_toks)\n",
    "            add_postags.append(short_postags[i])\n",
    "            add_nertags.append(short_nertags[i])\n",
    "            add_speechacts.append(short_speechacts[i])\n",
    "            add_topics.append(short_topics[i])\n",
    "            add_topic_acts.append(short_topic_acts[i])\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"done\", i, \"of\", len(short_speechacts))\n",
    "        \n",
    "len(add_tokens), len(add_postags), len(add_nertags), len(add_speechacts), len(add_topics), len(add_topic_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'it', \"'s\", 'it', \"'s\", 'a', 'long', 'good', 'itinerary']\n",
      "['CC', 'PRP', 'VBZ', 'PRP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['and', 'it', \"'s\", 'it', \"'s\", 'a', 'long', 'good', 'itinerary']\n",
      "['CC', 'PRP', 'VBZ', 'PRP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['could', 'i', 'have', 'front', 'facing', 'from', 'Incheon', 'to', 'erm']\n",
      "['MD', 'VB', 'VBP', 'VBN', 'VBG', 'IN', 'NNP', 'TO', 'VB']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'GEO', 'O', 'O']\n",
      "\n",
      "['could', 'i', 'have', 'front', 'facing', 'from', 'Paris', 'to', 'erm']\n",
      "['MD', 'VB', 'VBP', 'VBN', 'VBG', 'IN', 'NNP', 'TO', 'VB']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'GEO', 'O', 'O']\n",
      "\n",
      "['could', 'i', 'have', 'front', 'facing', 'from', 'Seoul', 'to', 'erm']\n",
      "['MD', 'VB', 'VBP', 'VBN', 'VBG', 'IN', 'NNP', 'TO', 'VB']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'GEO', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,105):\n",
    "    print(add_tokens[i])\n",
    "    print(add_postags[i])\n",
    "    print(add_nertags[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_speechacts += short_speechacts\n",
    "add_topics += short_topics\n",
    "add_topic_acts += short_topic_acts\n",
    "add_tokens += short_tokens\n",
    "add_postags += short_postags\n",
    "add_nertags += short_nertags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(lol):\n",
    "    result = []\n",
    "    for lst in lol:\n",
    "        this_sent = []\n",
    "        for w in lst:\n",
    "            for number in ['1','2','3','4','5','6','7','8','9','0']:\n",
    "                w = w.replace(number, '#')\n",
    "            this_sent.append(w.lower())\n",
    "        result.append(this_sent)\n",
    "    return result\n",
    "\n",
    "add_tokens = lowercase(add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "np.random.seed(1337)\n",
    "sindices = list(np.random.permutation(len(add_speechacts)))\n",
    "\n",
    "add_speechacts = [add_speechacts[i] for i in sindices]\n",
    "add_topics = [add_topics[i] for i in sindices]\n",
    "add_topic_acts = [add_topic_acts[i] for i in sindices]\n",
    "add_tokens = [add_tokens[i] for i in sindices]\n",
    "add_postags = [add_postags[i] for i in sindices]\n",
    "add_nertags = [add_nertags[i] for i in sindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7814, 11080, 34, 5276, 3897, 1221, 13246, 14619, 4540, 6547]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sindices[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vocab and index dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word = get_vocab(add_tokens, 10000)\n",
    "pos2idx, idx2pos = get_vocab(alltags, len(set([tag for sent in add_postags for tag in sent]))+2)\n",
    "ner2idx, idx2ner = get_vocab(allners, len(set([tag for sent in add_nertags for tag in sent]))+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../00_data/encoded/word2idx.npy', word2idx)\n",
    "np.save('../00_data/encoded/idx2word.npy', idx2word)\n",
    "np.save('../00_data/encoded/pos2idx.npy', pos2idx)\n",
    "np.save('../00_data/encoded/idx2pos.npy', idx2pos)\n",
    "np.save('../00_data/encoded/ner2idx.npy', ner2idx)\n",
    "np.save('../00_data/encoded/idx2ner.npy', idx2ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../00_data/encoded/add_speechacts.npy', add_speechacts)\n",
    "np.save('../00_data/encoded/add_topics.npy', add_topics)\n",
    "np.save('../00_data/encoded/add_topic_acts.npy', add_topic_acts)\n",
    "np.save('../00_data/encoded/add_tokens.npy', add_tokens)\n",
    "np.save('../00_data/encoded/add_postags.npy', add_postags)\n",
    "np.save('../00_data/encoded/add_nertags.npy', add_nertags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
